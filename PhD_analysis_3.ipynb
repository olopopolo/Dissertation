{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODWZhO0IRtB3U2mCNQut9m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olopopolo/Dissertation_repo/blob/main/PhD_analysis_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZq0x94awRop"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/mnt/data/ProgConstructions.xlsx'\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Filter out ambiguous cases\n",
        "filtered_data = data[data['construction_clean'] != 'ambiguous']\n",
        "\n",
        "# Calculate frequencies for 'construction_clean' and 'construction_type' without ambiguous cases\n",
        "filtered_construction_clean_freq = filtered_data['construction_clean'].value_counts().reset_index()\n",
        "filtered_construction_clean_freq.columns = ['construction_clean', 'frequency']\n",
        "\n",
        "filtered_construction_type_freq = filtered_data['construction_type'].value_counts().reset_index()\n",
        "filtered_construction_type_freq.columns = ['construction_type', 'frequency']\n",
        "\n",
        "# Calculate mean values for 'construction_clean' and 'construction_type' without ambiguous cases\n",
        "filtered_construction_clean_mean_table = filtered_data.groupby('construction_clean').size().mean()\n",
        "filtered_construction_type_mean_table = filtered_data.groupby('construction_type').size().mean()\n",
        "\n",
        "# Create a table for construction clean mean values\n",
        "construction_clean_mean_table = filtered_construction_clean_freq.copy()\n",
        "construction_clean_mean_table['mean_value'] = construction_clean_mean_table['frequency'] / construction_clean_mean_table['frequency'].sum() * filtered_construction_clean_mean_table\n",
        "\n",
        "# Create a table for construction type mean values\n",
        "construction_type_mean_table = filtered_construction_type_freq.copy()\n",
        "construction_type_mean_table['mean_value'] = construction_type_mean_table['frequency'] / construction_type_mean_table['frequency'].sum() * filtered_construction_type_mean_table\n",
        "\n",
        "# Add a percentage column to the mean tables\n",
        "# For construction clean\n",
        "total_clean = filtered_construction_clean_freq['frequency'].sum()\n",
        "construction_clean_mean_table['percentage'] = (construction_clean_mean_table['frequency'] / total_clean) * 100\n",
        "\n",
        "# For construction type\n",
        "total_type = filtered_construction_type_freq['frequency'].sum()\n",
        "construction_type_mean_table['percentage'] = (construction_type_mean_table['frequency'] / total_type) * 100\n",
        "\n",
        "# Visualization of percentages and raw frequencies with labels and different colors\n",
        "\n",
        "# Plot for 'construction_clean' percentages and raw frequencies\n",
        "plt.figure(figsize=(12, 6))\n",
        "bars = plt.bar(filtered_construction_clean_freq['construction_clean'], construction_clean_mean_table['percentage'], color='skyblue')\n",
        "plt.xlabel('Construction Clean')\n",
        "plt.ylabel('Percentage')\n",
        "plt.title('Percentages and Raw Frequencies of constructional schemas')\n",
        "plt.xticks(rotation=90)\n",
        "for i, bar in enumerate(bars):\n",
        "    yval = bar.get_height()\n",
        "    raw_freq = filtered_construction_clean_freq['frequency'][i]\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, yval + 1, f'{yval:.2f}%\\n({raw_freq})', ha='center', va='bottom')\n",
        "plt.show()\n",
        "\n",
        "# Plot for 'construction_type' percentages and raw frequencies\n",
        "plt.figure(figsize=(12, 6))\n",
        "bars = plt.bar(filtered_construction_type_freq['construction_type'], construction_type_mean_table['percentage'], color='lightcoral')\n",
        "plt.xlabel('Construction Type')\n",
        "plt.ylabel('Percentage')\n",
        "plt.title('Percentages and Raw Frequencies of construction types')\n",
        "plt.xticks(rotation=90)\n",
        "for i, bar in enumerate(bars):\n",
        "    yval = bar.get_height()\n",
        "    raw_freq = filtered_construction_type_freq['frequency'][i]\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.5, f'{yval:.2f}%\\n({raw_freq})', ha='center', va='bottom')\n",
        "plt.show()\n",
        "\n",
        "# Prepare the data for the network graph with weights based on frequencies\n",
        "connections = filtered_data.groupby(['construction_clean', 'construction_type']).size().reset_index(name='count')\n",
        "\n",
        "# Create a graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add edges to the graph with weights\n",
        "for _, row in connections.iterrows():\n",
        "    G.add_edge(row['construction_clean'], row['construction_type'], weight=row['count'])\n",
        "\n",
        "# Node sizes based on frequencies\n",
        "node_sizes = {}\n",
        "for node in G.nodes:\n",
        "    if node in filtered_construction_clean_freq['construction_clean'].values:\n",
        "        node_sizes[node] = filtered_construction_clean_freq.loc[filtered_construction_clean_freq['construction_clean'] == node, 'frequency'].values[0] * 10\n",
        "    else:\n",
        "        node_sizes[node] = filtered_construction_type_freq.loc[filtered_construction_type_freq['construction_type'] == node, 'frequency'].values[0] * 10\n",
        "\n",
        "# Define node colors\n",
        "node_colors = ['skyblue' if node in filtered_construction_clean_freq['construction_clean'].values else 'lightcoral' for node in G.nodes]\n",
        "\n",
        "# Get edge weights for thickness\n",
        "edge_weights = [G[u][v]['weight'] for u, v in G.edges]\n",
        "scaled_edge_weights = [weight * 0.05 for weight in edge_weights]\n",
        "\n",
        "# Draw the graph with a weighted layout and adjusted edge thickness\n",
        "plt.figure(figsize=(14, 10))\n",
        "pos = nx.spring_layout(G, k=0.5, weight='weight')\n",
        "nx.draw(G, pos, with_labels=True, node_size=[node_sizes[node] for node in G.nodes], node_color=node_colors, font_size=10, font_weight=\"bold\", edge_color=\"grey\", width=scaled_edge_weights)\n",
        "\n",
        "plt.title('Network graph between Constructional Schemas and Construction Types')\n",
        "plt.show()\n",
        "\n",
        "# List of unique constructional schemas\n",
        "construction_schemas = filtered_data['construction_clean'].unique()\n",
        "\n",
        "# Generate a network graph for each constructional schema\n",
        "for schema in construction_schemas:\n",
        "    # Filter data for the current schema\n",
        "    schema_data = filtered_data[filtered_data['construction_clean'] == schema]\n",
        "\n",
        "    # Prepare the data with weights based on frequencies\n",
        "    schema_connections = schema_data.groupby(['construction_clean', 'construction_type']).size().reset_index(name='count')\n",
        "\n",
        "    # Create a graph\n",
        "    G_schema = nx.DiGraph()\n",
        "\n",
        "    # Add edges to the graph with weights\n",
        "    for _, row in schema_connections.iterrows():\n",
        "        G_schema.add_edge(row['construction_clean'], row['construction_type'], weight=row['count'])\n",
        "\n",
        "    # Node sizes based on frequencies\n",
        "    schema_node_sizes = {}\n",
        "    for node in G_schema.nodes:\n",
        "        if node == schema:\n",
        "            schema_node_sizes[node] = filtered_construction_clean_freq.loc[filtered_construction_clean_freq['construction_clean'] == node, 'frequency'].values[0] * 10\n",
        "        else:\n",
        "            schema_node_sizes[node] = filtered_construction_type_freq.loc[filtered_construction_type_freq['construction_type'] == node, 'frequency'].values[0] * 10\n",
        "\n",
        "    # Define node colors\n",
        "    schema_node_colors = ['skyblue' if node == schema else 'lightcoral' for node in G_schema.nodes]\n",
        "\n",
        "    # Get edge weights for thickness\n",
        "    schema_edge_weights = [G_schema[u][v]['weight'] for u, v in G_schema.edges]\n",
        "    scaled_schema_edge_weights = [weight * 0.05 for weight in schema_edge_weights]\n",
        "\n",
        "    # Draw the graph with a weighted layout and adjusted edge thickness\n",
        "    plt.figure(figsize=(14, 10))\n",
        "    pos_schema = nx.spring_layout(G_schema, k=0.5, weight='weight')\n",
        "    nx.draw(G_schema, pos_schema, with_labels=True, node_size=[schema_node_sizes[node] for node in G_schema.nodes], node_color=schema_node_colors, font_size=10, font_weight=\"bold\", edge_color=\"grey\", width=scaled_schema_edge_weights)\n",
        "\n",
        "    plt.title(f'Network graph for Constructional Schema: {schema}')\n",
        "    plt.show()\n",
        "\n",
        "#create a network graph to connect 'construction_clean' with 'meaning'\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/mnt/data/ProgConstructions.xlsx'\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Filter out ambiguous cases\n",
        "filtered_data = data[data['construction_clean'] != 'ambiguous']\n",
        "\n",
        "# Calculate frequencies for 'construction_clean' and 'meaning' without ambiguous cases\n",
        "filtered_construction_clean_meaning_freq = filtered_data.groupby(['construction_clean', 'meaning']).size().reset_index(name='count')\n",
        "\n",
        "# Create a graph for meanings and constructional schemas\n",
        "G_meaning_reversed = nx.DiGraph()\n",
        "\n",
        "# Add edges to the graph with weights based on the frequency of connections (reversed direction)\n",
        "for _, row in filtered_construction_clean_meaning_freq.iterrows():\n",
        "    G_meaning_reversed.add_edge(row['meaning'], row['construction_clean'], weight=row['count'])\n",
        "\n",
        "# Node sizes based on frequencies (reversed)\n",
        "node_sizes_meaning_reversed = {}\n",
        "for node in G_meaning_reversed.nodes:\n",
        "    if node in filtered_construction_clean_meaning_freq['meaning'].values:\n",
        "        node_sizes_meaning_reversed[node] = filtered_construction_clean_meaning_freq[filtered_construction_clean_meaning_freq['meaning'] == node]['count'].sum() * 10\n",
        "    else:\n",
        "        node_sizes_meaning_reversed[node] = filtered_construction_clean_freq.loc[filtered_construction_clean_freq['construction_clean'] == node, 'frequency'].values[0] * 10\n",
        "\n",
        "# Define node colors (reversed)\n",
        "node_colors_meaning_reversed = ['lightgreen' if node in filtered_construction_clean_meaning_freq['meaning'].values else 'skyblue' for node in G_meaning_reversed.nodes]\n",
        "\n",
        "# Get edge weights for thickness (reversed)\n",
        "edge_weights_meaning_reversed = [G_meaning_reversed[u][v]['weight'] for u, v in G_meaning_reversed.edges]\n",
        "scaled_edge_weights_meaning_reversed = [weight * 0.05 for weight in edge_weights_meaning_reversed]\n",
        "\n",
        "# Draw the graph with a weighted layout and adjusted edge thickness\n",
        "plt.figure(figsize=(14, 10))\n",
        "pos_meaning_reversed = nx.spring_layout(G_meaning_reversed, k=1.5, iterations=100)  # Further increase k and iterations for more spacing\n",
        "nx.draw(G_meaning_reversed, pos_meaning_reversed, with_labels=True, node_size=[node_sizes_meaning_reversed[node] for node in G_meaning_reversed.nodes], node_color=node_colors_meaning_reversed, font_size=10, font_weight=\"bold\", edge_color=\"grey\", width=scaled_edge_weights_meaning_reversed)\n",
        "\n",
        "plt.title('Network graph of Meanings and Constructional Schemas')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a multinomial logistic regression model using sklearn\n",
        "log_reg_multinomial = LogisticRegression(multi_class='multinomial', max_iter=10000, solver='lbfgs')\n",
        "log_reg_multinomial.fit(X_scaled, y)\n",
        "\n",
        "# Get coefficients for each class\n",
        "coefficients = log_reg_multinomial.coef_\n",
        "\n",
        "# Create a DataFrame for coefficients\n",
        "coefficients_df = pd.DataFrame(coefficients.T, index=feature_names, columns=label_encoder.classes_)\n",
        "\n",
        "# Display the coefficients DataFrame\n",
        "tools.display_dataframe_to_user(name=\"Multinomial Logistic Regression Coefficients\", dataframe=coefficients_df)\n",
        "\n",
        "coefficients_df.head()\n",
        "\n",
        "# Calculate the absolute values of coefficients for better visualization\n",
        "coefficients_abs = coefficients_df.abs()\n",
        "\n",
        "# Get the maximum absolute coefficient value for scaling\n",
        "max_coeff_value = coefficients_abs.values.max()\n",
        "\n",
        "# Plot the feature importance for each meaning with the same x-axis scale\n",
        "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(18, 18))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, meaning in enumerate(coefficients_abs.columns):\n",
        "    sorted_features = coefficients_abs[meaning].sort_values(ascending=False)\n",
        "    axes[i].barh(sorted_features.index, sorted_features.values, color='skyblue')\n",
        "    axes[i].set_xlim(0, max_coeff_value)  # Set the same scale for the x-axis\n",
        "    axes[i].set_title(f'Feature Importance for {meaning}')\n",
        "    axes[i].set_xlabel('Absolute Coefficient Value')\n",
        "    axes[i].invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Plot the feature importance for each meaning with labels for the coefficients\n",
        "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(18, 18))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, meaning in enumerate(coefficients_abs.columns):\n",
        "    sorted_features = coefficients_abs[meaning].sort_values(ascending=False)\n",
        "    bars = axes[i].barh(sorted_features.index, sorted_features.values, color='skyblue')\n",
        "    axes[i].set_xlim(0, max_coeff_value)  # Set the same scale for the x-axis\n",
        "    axes[i].set_title(f'Feature Importance for {meaning}')\n",
        "    axes[i].set_xlabel('Absolute Coefficient Value')\n",
        "    axes[i].invert_yaxis()\n",
        "\n",
        "    # Add labels to the bars\n",
        "    for bar in bars:\n",
        "        width = bar.get_width()\n",
        "        label_x_pos = width + max_coeff_value * 0.01  # Offset the label slightly from the bar\n",
        "        axes[i].text(label_x_pos, bar.get_y() + bar.get_height() / 2, f'{width:.2f}', va='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XwYHHkKF0nXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Encode the categorical data for PCA\n",
        "encoded_data = pd.get_dummies(mca_data)\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca_transformed = pca.fit_transform(encoded_data)\n",
        "\n",
        "# Prepare the transformed data for plotting\n",
        "pca_transformed_df = pd.DataFrame(pca_transformed, columns=['Dimension 1', 'Dimension 2'])\n",
        "pca_transformed_df['meaning'] = filtered_data_clean['meaning'].values\n",
        "pca_transformed_df['construction_clean'] = filtered_data_clean['construction_clean'].values\n",
        "\n",
        "# Plot the PCA results\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.scatterplot(data=pca_transformed_df, x='Dimension 1', y='Dimension 2', hue='meaning', style='construction_clean', s=100)\n",
        "plt.title('Principal Component Analysis (PCA) of Meanings and Constructions')\n",
        "plt.xlabel('Dimension 1')\n",
        "plt.ylabel('Dimension 2')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "584LK2UD2x9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#improve the network graph for 'meaning' and 'construction_clean'\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Filter relevant columns\n",
        "data_filtered = data[['meaning', 'construction_clean']]\n",
        "\n",
        "# Create a network graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add nodes and edges\n",
        "for index, row in data_filtered.iterrows():\n",
        "    meaning = row['meaning']\n",
        "    construction = row['construction_clean']\n",
        "    G.add_node(meaning, label=meaning)\n",
        "    G.add_node(construction, label=construction)\n",
        "    G.add_edge(meaning, construction)\n",
        "\n",
        "# Draw the network graph\n",
        "plt.figure(figsize=(12, 8))\n",
        "pos = nx.spring_layout(G)\n",
        "nx.draw(G, pos, with_labels=True, node_size=3000, node_color=\"skyblue\", font_size=10, font_weight=\"bold\", edge_color=\"gray\")\n",
        "plt.title('Network Graph of Meanings and Constructions')\n",
        "plt.show()\n",
        "\n",
        "# Assign colors based on node type\n",
        "color_map = []\n",
        "for node in G:\n",
        "    if node in data_filtered['meaning'].values:\n",
        "        color_map.append('skyblue')  # Color for meanings\n",
        "    else:\n",
        "        color_map.append('lightgreen')  # Color for constructions\n",
        "\n",
        "# Draw the network graph with different colors\n",
        "plt.figure(figsize=(12, 8))\n",
        "pos = nx.spring_layout(G)\n",
        "nx.draw(G, pos, with_labels=True, node_size=3000, node_color=color_map, font_size=10, font_weight=\"bold\", edge_color=\"gray\")\n",
        "plt.title('Network Graph of Meanings and Constructions')\n",
        "plt.show()\n",
        "\n",
        "# Filter out ambiguous cases\n",
        "filtered_data = data_filtered[data_filtered['meaning'] != 'ambiguous']\n",
        "\n",
        "# Create a new network graph without ambiguous cases\n",
        "G_filtered = nx.Graph()\n",
        "\n",
        "# Add nodes and edges for the filtered data\n",
        "for index, row in filtered_data.iterrows():\n",
        "    meaning = row['meaning']\n",
        "    construction = row['construction_clean']\n",
        "    G_filtered.add_node(meaning, label=meaning)\n",
        "    G_filtered.add_node(construction, label=construction)\n",
        "    G_filtered.add_edge(meaning, construction)\n",
        "\n",
        "# Calculate degree centrality for the filtered graph\n",
        "degree_centrality_filtered = nx.degree_centrality(G_filtered)\n",
        "\n",
        "# Assign colors based on node type and scale sizes based on degree centrality\n",
        "color_map_filtered = []\n",
        "sizes = []\n",
        "for node in G_filtered:\n",
        "    if node in filtered_data['meaning'].values:\n",
        "        color_map_filtered.append('skyblue')  # Color for meanings\n",
        "    else:\n",
        "        color_map_filtered.append('lightgreen')  # Color for constructions\n",
        "    sizes.append(degree_centrality_filtered[node] * 3000)  # Scale size for better visualization\n",
        "\n",
        "# Draw the filtered network graph with degree centrality\n",
        "plt.figure(figsize=(12, 8))\n",
        "pos = nx.spring_layout(G_filtered)\n",
        "nx.draw(G_filtered, pos, with_labels=True, node_size=sizes, node_color=color_map_filtered, font_size=10, font_weight=\"bold\", edge_color=\"gray\")\n",
        "plt.title('Network Graph of Meanings and Constructions (Filtered)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GV-8bIDY3EyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the frequency (strength) of each meaning-construction pair\n",
        "pair_strengths = filtered_data_clean.groupby(['meaning', 'construction_clean']).size().reset_index(name='strength')\n",
        "\n",
        "# Display the table of strengths between meanings and constructions\n",
        "tools.display_dataframe_to_user(name=\"Strengths Between Meanings and Constructions\", dataframe=pair_strengths)\n",
        "\n",
        "pair_strengths.head()\n",
        "\n",
        "\n",
        "# Calculate the total occurrences\n",
        "total_occurrences = pair_strengths['strength'].sum()\n",
        "\n",
        "# Calculate normalized strength\n",
        "pair_strengths['normalized_strength'] = pair_strengths['strength'] / total_occurrences\n",
        "\n",
        "# Display the updated table with normalized strength\n",
        "tools.display_dataframe_to_user(name=\"Normalized Strengths Between Meanings and Constructions\", dataframe=pair_strengths)\n",
        "\n",
        "pair_strengths.head()\n",
        "\n",
        "from scipy.stats import chi2_contingency\n",
        "import numpy as np\n",
        "\n",
        "# Create a contingency table\n",
        "contingency_table = pd.crosstab(filtered_data_clean['meaning'], filtered_data_clean['construction_clean'])\n",
        "\n",
        "# Perform Chi-Square test\n",
        "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "# Calculate Cramér's V\n",
        "n = np.sum(contingency_table.values)\n",
        "cramers_v = np.sqrt(chi2 / (n * (min(contingency_table.shape) - 1)))\n",
        "\n",
        "# Prepare the results\n",
        "stats_results = {\n",
        "    \"Chi-Square Statistic\": chi2,\n",
        "    \"p-value\": p,\n",
        "    \"Degrees of Freedom\": dof,\n",
        "    \"Cramér's V\": cramers_v\n",
        "}\n",
        "\n",
        "stats_results_df = pd.DataFrame(list(stats_results.items()), columns=['Statistic', 'Value'])\n",
        "\n",
        "tools.display_dataframe_to_user(name=\"Chi-Square Test Results\", dataframe=stats_results_df)\n",
        "\n",
        "stats_results_df\n",
        "\n",
        "# Calculate expected frequencies and residuals\n",
        "observed = contingency_table.values\n",
        "expected = expected.astype(int)\n",
        "\n",
        "# Calculate residuals\n",
        "residuals = observed - expected\n",
        "\n",
        "# Create a DataFrame for the residuals\n",
        "residuals_df = pd.DataFrame(residuals, index=contingency_table.index, columns=contingency_table.columns)\n",
        "\n",
        "# Identify significant pairs based on standardized residuals\n",
        "# Residuals are considered significant if their absolute value is greater than 2\n",
        "significant_pairs = residuals_df[residuals_df.abs() > 2].stack().reset_index()\n",
        "significant_pairs.columns = ['Meaning', 'Construction', 'Residual']\n",
        "\n",
        "tools.display_dataframe_to_user(name=\"Significant Construction-Meaning Pairs\", dataframe=significant_pairs)\n",
        "\n",
        "significant_pairs.head()\n",
        "\n",
        "# Create a mask for significant residuals\n",
        "mask = residuals_df.abs() <= 2\n",
        "\n",
        "# Generate a heatmap of the residuals, highlighting significant pairs\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(residuals_df, annot=True, mask=mask, cmap=\"coolwarm\", center=0, cbar_kws={'label': 'Residuals'})\n",
        "plt.title('Heatmap of Residuals Highlighting Significant Construction-Meaning Pairs')\n",
        "plt.xlabel('Construction')\n",
        "plt.ylabel('Meaning')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "uNgfWGYq3f7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the frequencies for the aspectotemporal meanings and intersubjective connotations\n",
        "aspectotemporal_frequencies = data['meaning'].apply(lambda x: x if x in aspectotemporal_meanings else None).value_counts()\n",
        "intersubjective_frequencies = data['meaning'].apply(lambda x: x if x in intersubjective_connotations else None).value_counts()\n",
        "\n",
        "# Create a network graph with scaled nodes and an additional superordinate concept\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add superordinate node\n",
        "G.add_node(\"epistemic contingency\", group='superordinate')\n",
        "\n",
        "# Add nodes with frequencies as their sizes\n",
        "for meaning, freq in aspectotemporal_frequencies.items():\n",
        "    G.add_node(meaning, group='aspectotemporal', size=freq)\n",
        "\n",
        "for meaning, freq in intersubjective_frequencies.items():\n",
        "    G.add_node(meaning, group='intersubjective', size=freq)\n",
        "\n",
        "# Add central nodes\n",
        "G.add_node(\"aspectotemporal meanings\", group='core', size=aspectotemporal_frequencies.sum())\n",
        "G.add_node(\"intersubjective connotations\", group='core', size=intersubjective_frequencies.sum())\n",
        "\n",
        "# Add edges\n",
        "for meaning in aspectotemporal_frequencies.index:\n",
        "    G.add_edge(meaning, \"aspectotemporal meanings\")\n",
        "\n",
        "for meaning in intersubjective_frequencies.index:\n",
        "    G.add_edge(meaning, \"intersubjective connotations\")\n",
        "\n",
        "# Connect core nodes to the superordinate concept\n",
        "G.add_edge(\"aspectotemporal meanings\", \"epistemic contingency\")\n",
        "G.add_edge(\"intersubjective connotations\", \"epistemic contingency\")\n",
        "\n",
        "# Position nodes\n",
        "pos = nx.spring_layout(G, k=0.5)\n",
        "\n",
        "# Draw the graph with scaled nodes\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "# Draw nodes with colors based on their group and sizes based on frequencies\n",
        "colors = [G.nodes[node]['group'] for node in G.nodes]\n",
        "sizes = [G.nodes[node].get('size', 100) * 50 for node in G.nodes]  # Scale sizes for better visibility\n",
        "\n",
        "color_map = {\n",
        "    'aspectotemporal': 'skyblue',\n",
        "    'intersubjective': 'lightcoral',\n",
        "    'core': 'yellow',\n",
        "    'superordinate': 'green'\n",
        "}\n",
        "node_colors = [color_map[color] for color in colors]\n",
        "\n",
        "nx.draw(G, pos, with_labels=True, node_color=node_colors, edge_color='gray', node_size=sizes, font_size=10, font_weight='bold')\n",
        "\n",
        "plt.title('Network Graph of Meaning Types with Epistemic Contingency')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wZxF3s6Xdeq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'Path_to_Your_File.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Extract relevant columns\n",
        "construction_types = data['construction_type']\n",
        "construction_clean = data['construction_clean']\n",
        "\n",
        "# Create a DataFrame with the frequency of each pair (construction_clean, construction_type)\n",
        "edges = data.groupby(['construction_clean', 'construction_type']).size().reset_index(name='weight')\n",
        "\n",
        "# Create the graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add nodes for construction_clean and construction_type with frequencies\n",
        "node_sizes = {}\n",
        "for node in construction_clean:\n",
        "    if node not in node_sizes:\n",
        "        node_sizes[node] = construction_clean.value_counts()[node]\n",
        "for node in construction_types:\n",
        "    if node not in node_sizes:\n",
        "        node_sizes[node] = construction_types.value_counts()[node]\n",
        "\n",
        "# Add nodes with attributes\n",
        "duplicate_nodes = set(construction_clean) & set(construction_types)\n",
        "for node, size in node_sizes.items():\n",
        "    if node in construction_clean.unique():\n",
        "        G.add_node(node, size=size, color='blue')\n",
        "    elif node in duplicate_nodes:\n",
        "        G.add_node(node + '_type', size=size, color='red')\n",
        "    else:\n",
        "        G.add_node(node, size=size, color='red')\n",
        "\n",
        "# Add edges with weights\n",
        "for _, row in edges.iterrows():\n",
        "    if row['construction_type'] in duplicate_nodes:\n",
        "        G.add_edge(row['construction_clean'], row['construction_type'] + '_type', weight=row['weight'])\n",
        "    else:\n",
        "        G.add_edge(row['construction_clean'], row['construction_type'], weight=row['weight'])\n",
        "\n",
        "# Set up the plot size and remove the grid\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.axis('off')  # This removes the background grid\n",
        "\n",
        "# Get positions for the nodes with more frequent nodes more central\n",
        "pos_central = nx.spring_layout(G, k=0.5, center=[0.5, 0.5])\n",
        "\n",
        "# Draw the nodes with attributes\n",
        "sizes = [G.nodes[node].get('size', 10)*10 for node in G.nodes()]\n",
        "colors = [G.nodes[node].get('color', 'black') for node in G.nodes()]\n",
        "nx.draw_networkx_nodes(G, pos_central, node_size=sizes, node_color=colors)\n",
        "\n",
        "# Draw the edges with adjusted weights\n",
        "weights = [G[u][v].get('weight', 1) for u, v in G.edges()]\n",
        "scaled_weights = [w/50 for w in weights]\n",
        "nx.draw_networkx_edges(G, pos_central, width=scaled_weights)\n",
        "\n",
        "# Draw the labels with further offset to avoid overlapping\n",
        "labels = {node: node.replace('_type', '') for node in G.nodes()}\n",
        "nx.draw_networkx_labels(G, pos_central, labels=labels, font_size=8, verticalalignment='center', horizontalalignment='right')\n",
        "\n",
        "# Update the title\n",
        "plt.title('Network graph of constructional schemas and construction types')\n",
        "\n",
        "# Save to PDF\n",
        "pdf_file_path = 'Network_Graph_Constructional_Schemas_and_Types.pdf'\n",
        "plt.savefig(pdf_file_path, format='pdf', bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(f\"Graph saved as {pdf_file_path}\")\n"
      ],
      "metadata": {
        "id": "b8e2Jlu8Evg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/mnt/data/ProgConstructionsMerged.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the dataset to understand its structure\n",
        "data.head()\n",
        "\n",
        "# Create a new column combining 'construction_type', 'construction_clean', and 'meaning'\n",
        "data['symbolic_relation'] = data.apply(lambda x: f\"{x['construction_type']} ({x['construction_clean']}): {x['meaning']}\", axis=1)\n",
        "\n",
        "# Display the new column and the relevant original columns for comparison\n",
        "data[['construction_type', 'construction_clean', 'meaning', 'symbolic_relation']].head()\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the frequency of each symbolic relation\n",
        "frequency_table = data['symbolic_relation'].value_counts().reset_index()\n",
        "frequency_table.columns = ['Symbolic Relation', 'Frequency']\n",
        "\n",
        "# Calculate relative frequencies\n",
        "total_entries = len(data)\n",
        "frequency_table['Relative Frequency'] = frequency_table['Frequency'] / total_entries * 100\n",
        "\n",
        "# Display the frequency table\n",
        "frequency_table\n",
        "\n",
        "# Prepare data for heatmap\n",
        "heatmap_data = data.pivot_table(index='construction_clean', columns='meaning', values='symbolic_relation', aggfunc='count', fill_value=0)\n",
        "\n",
        "# Plot heatmap for absolute frequencies\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(heatmap_data, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\n",
        "plt.title('Heatmap of Absolute Frequencies of Symbolic Relations')\n",
        "plt.ylabel('Construction Form (Clean)')\n",
        "plt.xlabel('Meaning')\n",
        "plt.show()\n",
        "\n",
        "# Plot heatmap for relative frequencies\n",
        "heatmap_data_relative = heatmap_data.div(heatmap_data.sum().sum()).multiply(100)\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(heatmap_data_relative, annot=True, fmt=\".2f\", cmap=\"YlGnBu\")\n",
        "plt.title('Heatmap of Relative Frequencies of Symbolic Relations')\n",
        "plt.ylabel('Construction Form (Clean)')\n",
        "plt.xlabel('Meaning')\n",
        "plt.show()\n",
        "\n",
        "from scipy.stats import chi2_contingency\n",
        "import numpy as np\n",
        "\n",
        "# Prepare the data excluding 'ambiguous' entries\n",
        "analysis_data = data[(data['construction_clean'] != 'ambiguous') & (data['meaning'] != 'ambiguous')]\n",
        "\n",
        "# Create a contingency table\n",
        "contingency_table = pd.crosstab(analysis_data['construction_clean'], analysis_data['meaning'])\n",
        "\n",
        "# Perform the Chi-Square Test of Independence\n",
        "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "chi2, p_value, dof\n",
        "\n",
        "# Calculate contribution of each construction to each meaning as a percentage of the total for that meaning\n",
        "contribution_percentages = contingency_table.div(contingency_table.sum(axis=0), axis=1) * 100\n",
        "\n",
        "# Display the contribution percentages in a readable format\n",
        "contribution_percentages.transpose()\n",
        "\n",
        "# Plotting the contribution percentages as a heatmap for visual representation\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(contribution_percentages.transpose(), annot=True, fmt=\".2f\", cmap=\"coolwarm\", linewidths=.5)\n",
        "plt.title('Contribution of Each Construction to Certain Meanings')\n",
        "plt.xlabel('Construction Schema')\n",
        "plt.ylabel('Meaning')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the visual representation to a PDF\n",
        "visual_contribution_pdf = \"/mnt/data/Visual_Contribution_Heatmap.pdf\"\n",
        "plt.savefig(visual_contribution_pdf)\n",
        "plt.show()\n",
        "\n",
        "visual_contribution_pdf\n",
        "\n",
        "# Calculate the standardized residuals (contributions to chi-squared)\n",
        "residuals = (contingency_table - expected) / np.sqrt(expected)\n",
        "\n",
        "# Plotting the standardized residuals as a heatmap\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(residuals.transpose(), annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0, linewidths=.5)\n",
        "plt.title('Standardized Residuals for Chi-Squared Contributions')\n",
        "plt.xlabel('Construction Schema')\n",
        "plt.ylabel('Meaning')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the residuals heatmap to a PDF\n",
        "residuals_heatmap_pdf = \"/mnt/data/Residuals_Chi_Squared_Contribution.pdf\"\n",
        "plt.savefig(residuals_heatmap_pdf)\n",
        "plt.show()\n",
        "\n",
        "residuals_heatmap_pdf\n",
        "\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Initialize a DataFrame to store the chi-square results\n",
        "chi_square_results = pd.DataFrame(columns=['Construction', 'Meaning', 'Chi-Square Statistic', 'p-value', 'Degrees of Freedom'])\n",
        "\n",
        "# Iterate over each construction schema and meaning to perform chi-square tests\n",
        "for construction in contingency_table.index:\n",
        "    for meaning in contingency_table.columns:\n",
        "        # Construct the contingency table for the pair\n",
        "        observed = pd.crosstab(data['construction_clean'] == construction, data['meaning'] == meaning)\n",
        "\n",
        "        # Perform the chi-square test\n",
        "        chi2, p, dof, _ = chi2_contingency(observed)\n",
        "\n",
        "        # Store the results\n",
        "        chi_square_results = chi_square_results.append({\n",
        "            'Construction': construction,\n",
        "            'Meaning': meaning,\n",
        "            'Chi-Square Statistic': chi2,\n",
        "            'p-value': p,\n",
        "            'Degrees of Freedom': dof\n",
        "        }, ignore_index=True)\n",
        "\n",
        "# Display the results in a table format\n",
        "chi_square_results.head(len(chi_square_results))  # Showing all results could be large, adjust as needed\n",
        "\n",
        "# Calculate Cramer's V for each pair and store the results in a DataFrame\n",
        "def cramers_v(chi2, n):\n",
        "    return np.sqrt(chi2 / (n * min(contingency_table.shape) - 1))\n",
        "\n",
        "# Initialize a DataFrame to store Cramer's V results\n",
        "cramers_v_results = pd.DataFrame(columns=contingency_table.columns, index=contingency_table.index)\n",
        "\n",
        "# Iterate over each construction schema and meaning to calculate Cramer's V\n",
        "for construction in contingency_table.index:\n",
        "    for meaning in contingency_table.columns:\n",
        "        # Construct the contingency table for the pair\n",
        "        observed = pd.crosstab(data['construction_clean'] == construction, data['meaning'] == meaning)\n",
        "\n",
        "        # Perform the chi-square test\n",
        "        chi2, p, dof, _ = chi2_contingency(observed)\n",
        "\n",
        "        # Calculate Cramer's V\n",
        "        cramers_v_value = cramers_v(chi2, observed.sum().sum())\n",
        "        cramers_v_results.loc[construction, meaning] = cramers_v_value\n",
        "\n",
        "# Convert the results to float\n",
        "cramers_v_results = cramers_v_results.astype(float)\n",
        "\n",
        "# Display the results in a table format\n",
        "cramers_v_results\n",
        "\n",
        "# Calculate Cramer's V for each pair and store the results in a DataFrame\n",
        "def cramers_v(chi2, n):\n",
        "    return np.sqrt(chi2 / (n * min(contingency_table.shape) - 1))\n",
        "\n",
        "# Initialize a DataFrame to store Cramer's V results\n",
        "cramers_v_results = pd.DataFrame(columns=contingency_table.columns, index=contingency_table.index)\n",
        "\n",
        "# Iterate over each construction schema and meaning to calculate Cramer's V\n",
        "for construction in contingency_table.index:\n",
        "    for meaning in contingency_table.columns:\n",
        "        # Construct the contingency table for the pair\n",
        "        observed = pd.crosstab(data['construction_clean'] == construction, data['meaning'] == meaning)\n",
        "\n",
        "        # Perform the chi-square test\n",
        "        chi2, p, dof, _ = chi2_contingency(observed)\n",
        "\n",
        "        # Calculate Cramer's V\n",
        "        cramers_v_value = cramers_v(chi2, observed.sum().sum())\n",
        "        cramers_v_results.loc[construction, meaning] = cramers_v_value\n",
        "\n",
        "# Convert the results to float\n",
        "cramers_v_results = cramers_v_results.astype(float)\n",
        "\n",
        "# Display the results in a table format\n",
        "cramers_v_results\n"
      ],
      "metadata": {
        "id": "Dyn73y2FQV16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import chi2_contingency\n",
        "import numpy as np\n",
        "\n",
        "# Filtering out ambiguous entries\n",
        "filtered_data = data[(data['construction_clean'] != 'ambiguous') & (data['meaning'] != 'ambiguous')]\n",
        "\n",
        "# Creating a contingency table for each pair of construction_clean and meaning\n",
        "contingency_tables = {}\n",
        "chi2_results = []\n",
        "\n",
        "# Get unique values after filtering\n",
        "construction_types = filtered_data['construction_clean'].unique()\n",
        "meanings = filtered_data['meaning'].unique()\n",
        "\n",
        "for construction in construction_types:\n",
        "    for meaning in meanings:\n",
        "        # Creating a contingency table for each pair\n",
        "        contingency = pd.crosstab(filtered_data['construction_clean'] == construction,\n",
        "                                  filtered_data['meaning'] == meaning)\n",
        "        contingency_tables[(construction, meaning)] = contingency\n",
        "\n",
        "        # Performing Chi-squared test\n",
        "        chi2, p, dof, expected = chi2_contingency(contingency)\n",
        "        chi2_results.append({\n",
        "            'Construction': construction,\n",
        "            'Meaning': meaning,\n",
        "            'Chi2 Statistic': chi2,\n",
        "            'p-value': p,\n",
        "            'Significance': p < 0.05,  # Consider significance at alpha = 0.05\n",
        "            'Observed Frequency': contingency.iloc[1, 1],  # Frequency of pair co-occurrence\n",
        "            'Expected Frequency': expected[1, 1]  # Expected frequency if independent\n",
        "        })\n",
        "\n",
        "# Creating a DataFrame for results\n",
        "chi2_results_df = pd.DataFrame(chi2_results)\n",
        "chi2_results_df.sort_values('p-value', inplace=True)  # Sort by p-value for highlighting significance\n",
        "\n",
        "chi2_results_df.head(10), chi2_results_df.tail(10)  # Showing top and bottom 10 results for brevity\n"
      ],
      "metadata": {
        "id": "empv6YZkkIUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "\n",
        "# Creating a Decision Tree model\n",
        "dtree = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
        "\n",
        "# Fitting the model\n",
        "dtree.fit(X[['construction_encoded', 'adv_function_encoded']], y)\n",
        "\n",
        "# Plotting the decision tree\n",
        "plt.figure(figsize=(20, 10))\n",
        "tree_plot = plot_tree(dtree, feature_names=['construction_encoded', 'adv_function_encoded'],\n",
        "                      class_names=le_meaning.classes_, filled=True, rounded=True, fontsize=12)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Br01qs39lCGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.multitest import multipletests\n",
        "from itertools import combinations\n",
        "\n",
        "# List of unique school languages\n",
        "school_languages = data['author school language'].unique()\n",
        "\n",
        "# Initialize a list to store the pairwise comparison results\n",
        "pairwise_results = []\n",
        "\n",
        "# Perform pairwise chi-square tests for each constructional schema\n",
        "for construction in construction_types:\n",
        "    for lang1, lang2 in combinations(school_languages, 2):\n",
        "        contingency_table = pd.crosstab(\n",
        "            data[data['author school language'].isin([lang1, lang2])]['author school language'],\n",
        "            data[data['author school language'].isin([lang1, lang2])]['construction_clean'] == construction\n",
        "        )\n",
        "        chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
        "        pairwise_results.append({\n",
        "            'Construction': construction,\n",
        "            'Language Pair': f'{lang1} vs {lang2}',\n",
        "            'Chi-square': chi2,\n",
        "            'p-value': p\n",
        "        })\n",
        "\n",
        "# Convert results to a DataFrame\n",
        "pairwise_results_df = pd.DataFrame(pairwise_results)\n",
        "\n",
        "# Apply multiple testing correction (Bonferroni correction)\n",
        "corrected_pvals = multipletests(pairwise_results_df['p-value'], method='bonferroni')\n",
        "pairwise_results_df['corrected p-value'] = corrected_pvals[1]\n",
        "\n",
        "tools.display_dataframe_to_user(name=\"Pairwise Chi-square Test Results for Each Constructional Schema\", dataframe=pairwise_results_df)\n",
        "\n",
        "pairwise_results_df.sort_values(by='corrected p-value').head()\n",
        "\n",
        "# List of unique clusters\n",
        "clusters = data['clusters_olga'].unique()\n",
        "\n",
        "# Initialize a list to store the pairwise comparison results\n",
        "pairwise_results_clusters = []\n",
        "\n",
        "# Perform pairwise chi-square tests for each construction schema\n",
        "for construction in construction_types:\n",
        "    for cluster1, cluster2 in combinations(clusters, 2):\n",
        "        contingency_table = pd.crosstab(\n",
        "            data[data['clusters_olga'].isin([cluster1, cluster2])]['clusters_olga'],\n",
        "            data[data['clusters_olga'].isin([cluster1, cluster2])]['construction_clean'] == construction\n",
        "        )\n",
        "        chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
        "        pairwise_results_clusters.append({\n",
        "            'Construction': construction,\n",
        "            'Cluster Pair': f'{cluster1} vs {cluster2}',\n",
        "            'Chi-square': chi2,\n",
        "            'p-value': p\n",
        "        })\n",
        "\n",
        "# Convert results to a DataFrame\n",
        "pairwise_results_clusters_df = pd.DataFrame(pairwise_results_clusters)\n",
        "\n",
        "# Apply multiple testing correction (Bonferroni correction)\n",
        "corrected_pvals_clusters = multipletests(pairwise_results_clusters_df['p-value'], method='bonferroni')\n",
        "pairwise_results_clusters_df['corrected p-value'] = corrected_pvals_clusters[1]\n",
        "\n",
        "tools.display_dataframe_to_user(name=\"Pairwise Chi-square Test Results for Clusters Olga\", dataframe=pairwise_results_clusters_df)\n",
        "\n",
        "pairwise_results_clusters_df.sort_values(by='corrected p-value').head()\n",
        "\n"
      ],
      "metadata": {
        "id": "YKLdvNxucXa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Load your data\n",
        "data = pd.read_csv('/mnt/data/ProgConstructionsMerged.csv')\n",
        "\n",
        "# Filter out the \"ambiguous\" category\n",
        "filtered_data_no_ambiguous = data[data['construction_clean'] != 'ambiguous']\n",
        "\n",
        "# Group by author_id and construction_clean to see individual contributions\n",
        "author_contributions = filtered_data_no_ambiguous.groupby(['author_id', 'construction_clean']).size().unstack().fillna(0)\n",
        "\n",
        "# Standardize the data for PCA and clustering\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(author_contributions)\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca_result = pca.fit_transform(scaled_data)\n",
        "\n",
        "# Perform K-means clustering on the standardized data\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "clusters = kmeans.fit_predict(scaled_data)\n",
        "\n",
        "# Add cluster information back to author_contributions DataFrame\n",
        "author_contributions['Cluster'] = clusters\n",
        "\n",
        "# Map the cluster labels back to the original data\n",
        "filtered_data_no_ambiguous['Cluster'] = filtered_data_no_ambiguous['author_id'].map(author_contributions['Cluster'])\n",
        "\n",
        "# Ensure we are selecting the correct columns for constructional schemas in the filtered data\n",
        "schema_columns = ['V+ADV', 'V1+and+V1', 'Vaux+V', 'VingGER', 'aspectual+V', 'beim+Vinf+sein', 'find+V', 'perception+V']\n",
        "\n",
        "# Add schema columns to the filtered data by merging with author_contributions\n",
        "filtered_data_with_schemas = filtered_data_no_ambiguous.merge(author_contributions[schema_columns], left_on='author_id', right_index=True, how='left')\n",
        "\n",
        "# Calculate the average usage of each constructional schema within each cluster\n",
        "cluster_means_corrected = filtered_data_with_schemas.groupby('Cluster')[schema_columns].mean()\n",
        "\n",
        "# Visualize the PCA results with clustering information\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.scatterplot(x=pca_result[:, 0], y=pca_result[:, 1], hue=clusters, palette='viridis', legend='full')\n",
        "plt.title('PCA of Authors Based on Constructional Schema Usage with Cluster Information')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.legend(title='Cluster')\n",
        "plt.show()\n",
        "\n",
        "# Visualize the cluster means for constructional schemas\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.heatmap(cluster_means_corrected.T, annot=True, cmap='YlGnBu', cbar=True)\n",
        "plt.title('Cluster Means for Constructional Schemas')\n",
        "plt.xlabel('Cluster')\n",
        "plt.ylabel('Construction Schema')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oETb6mepYJDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a network graph with branching principles\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reload the data\n",
        "file_path = '/mnt/data/ProgConstructionsMerged.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Extract relevant columns for analysis\n",
        "meaning_data = data['meaning']\n",
        "\n",
        "# Calculate frequencies of each meaning\n",
        "meaning_frequencies = meaning_data.value_counts()\n",
        "\n",
        "# Define main meaning and its subtypes\n",
        "main_meaning = 'ongoingness'\n",
        "aspecto_temporal = ['orientation', 'virtual ongoingness', 'duration', 'temporary validity', 'habitual']\n",
        "intersubjective_connotations = ['emphasis', 'modal']\n",
        "\n",
        "# Define node sizes based on frequencies\n",
        "node_sizes = {meaning: meaning_frequencies[meaning] * 10 for meaning in meaning_frequencies.index}\n",
        "\n",
        "# Create a new graph for the updated layout\n",
        "G_final = nx.DiGraph()\n",
        "\n",
        "# Add nodes for all subtypes and the main meaning\n",
        "for subtype in subtypes:\n",
        "    G_final.add_node(subtype, size=node_sizes[subtype])\n",
        "G_final.add_node(main_meaning, size=node_sizes[main_meaning])\n",
        "\n",
        "# Add edges for the main meaning and subtypes with the corrected branching principles\n",
        "# Singular and actual: orientation, duration, temporary validity\n",
        "singular_actual = ['orientation', 'duration', 'temporary validity']\n",
        "for subtype in singular_actual:\n",
        "    G_final.add_edge('ongoingness', subtype, branching='singular and actual')\n",
        "# Connect nodes within this principle\n",
        "for i in range(len(singular_actual) - 1):\n",
        "    for j in range(i + 1, len(singular_actual)):\n",
        "        G_final.add_edge(singular_actual[i], singular_actual[j], branching='singular and actual')\n",
        "\n",
        "# Virtual and actual: virtual ongoingness\n",
        "G_final.add_edge('ongoingness', 'virtual ongoingness', branching='virtual and actual')\n",
        "\n",
        "# Multiple and virtual: habitual\n",
        "G_final.add_edge('ongoingness', 'habitual', branching='multiple and virtual')\n",
        "\n",
        "# Non-temporal: modal, emphasis\n",
        "non_temporal = ['modal', 'emphasis']\n",
        "for subtype in non_temporal:\n",
        "    G_final.add_edge('ongoingness', subtype, branching='non-temporal')\n",
        "# Connect nodes within this principle\n",
        "G_final.add_edge('modal', 'emphasis', branching='non-temporal')\n",
        "\n",
        "# Add dotted line connection from virtual ongoingness to habitual with label 'virtual'\n",
        "G_final.add_edge('virtual ongoingness', 'habitual', branching='virtual', style='dotted')\n",
        "\n",
        "# Define colors for the groups\n",
        "node_colors = {\n",
        "    **{node: 'lightgreen' for node in aspecto_temporal},\n",
        "    **{node: 'lightcoral' for node in intersubjective_connotations},\n",
        "    'ongoingness': 'skyblue'\n",
        "}\n",
        "\n",
        "# Draw the final graph with labeled edges and dotted line\n",
        "plt.figure(figsize=(16, 16))\n",
        "pos_final = nx.circular_layout(G_final)\n",
        "\n",
        "colors_final = [node_colors[node] for node in G_final.nodes]\n",
        "sizes_final = [node_sizes[node] for node in G_final.nodes]\n",
        "\n",
        "# Draw nodes\n",
        "nx.draw_networkx_nodes(G_final, pos_final, node_size=sizes_final, node_color=colors_final)\n",
        "\n",
        "# Draw solid edges\n",
        "solid_edges = [(u, v) for u, v, d in G_final.edges(data=True) if d.get('style') != 'dotted']\n",
        "nx.draw_networkx_edges(G_final, pos_final, edgelist=solid_edges, edge_color='gray')\n",
        "\n",
        "# Draw dotted edges\n",
        "dotted_edges = [(u, v) for u, v, d in G_final.edges(data=True) if d.get('style') == 'dotted']\n",
        "nx.draw_networkx_edges(G_final, pos_final, edgelist=dotted_edges, edge_color='gray', style='dotted')\n",
        "\n",
        "# Draw labels\n",
        "nx.draw_networkx_labels(G_final, pos_final, font_size=10, font_family=\"sans-serif\", verticalalignment='bottom')\n",
        "\n",
        "# Draw edge labels for branching principles\n",
        "edge_labels = nx.get_edge_attributes(G_final, 'branching')\n",
        "nx.draw_networkx_edge_labels(G_final, pos_final, edge_labels=edge_labels, font_size=8)\n",
        "\n",
        "# Add the legend\n",
        "plt.legend(handles=legend_elements, loc='upper right')\n",
        "plt.title('Network Graph of Meaning with Corrected Branching Principles')\n",
        "\n",
        "# Save the plot as a PNG file\n",
        "file_path_png = '/mnt/data/network_graph_meaning.png'\n",
        "plt.savefig(file_path_png)\n",
        "``` &#8203;:citation[oaicite:0]{index=0}&#8203;\n"
      ],
      "metadata": {
        "id": "ZMs-OLgdqrJo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}